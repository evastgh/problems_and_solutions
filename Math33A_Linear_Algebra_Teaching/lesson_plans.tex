\documentclass[12pt,a4paper]{article}
	%[fleqn] %%% --to make all equation left-algned--

% \usepackage[utf8]{inputenc}
% \DeclareUnicodeCharacter{1D12A}{\doublesharp}
% \DeclareUnicodeCharacter{2693}{\anchor}
% \usepackage{dingbat}
% \DeclareRobustCommand\dash\unskip\nobreak\thinspace{\textemdash\allowbreak\thinspace\ignorespaces}
\usepackage[top=2in, bottom=1in, left=1in, right=1in]{geometry}
%\usepackage{fullpage}

\usepackage{fancyhdr}\pagestyle{fancy}\rhead{Stephanie Wang}\lhead{Math 33A: Lesson Plans}

\usepackage{nicefrac, soul}


\usepackage{amsmath,amssymb,amsthm,amsfonts,microtype,stmaryrd}
	%{mathtools,wasysym,yhmath}

\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\fgreen}[1]{\textcolor{ForestGreen}{#1}}

\usepackage{mdframed}
	%\newtheorem{mdexample}{Example}
	\definecolor{warmgreen}{rgb}{0.8,0.9,0.85}
	% --Example:
	% \begin{center}
	% \begin{minipage}{0.7\textwidth}
	% \begin{mdframed}[backgroundcolor=warmgreen, 
	% skipabove=4pt,skipbelow=4pt,hidealllines=true, 
	% topline=false,leftline=false,middlelinewidth=10pt, 
	% roundcorner=10pt] 
	%%%% --CONTENTS-- %%%%
	% \end{mdframed}\end{minipage}\end{center}	

%\usepackage{graphicx} \graphicspath{ {/path/} }
	% --Example:
	% \includegraphics[scale=0.5]{picture name}
%\usepackage{caption} %%% --some awful package to make caption...

%\usepackage{hyperref}\hypersetup{linktocpage,colorlinks}\hypersetup{citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}

%%% --Text Fonts
%\usepackage{times} %%% --Times New Roman for LaTeX
%\usepackage{fontspec}\setmainfont{Times New Roman} %%% --Times New Roman; XeLaTeX only

%%% --Math Fonts
\renewcommand{\v}[1]{\ifmmode\mathbf{#1}\fi}
%\renewcommand{\mbf}[1]{\mathbf{#1}} %%% --vector
%\newcommand{\ca}[1]{\mathcal{#1}} %%% --"bigO"
%\newcommand{\bb}[1]{\mathbb{#1}} %%% --"Natural, Real numbers"
%\newcommand{\rom}[1]{\romannumeral{#1}} %%% --Roman numbers

%%% --Quick Arrows
\newcommand{\ra}[1]{\ifnum #1=1\rightarrow\fi\ifnum #1=2\Rightarrow\fi\ifnum #1=3\Rrightarrow\fi\ifnum #1=4\rightrightarrows\fi\ifnum #1=5\rightleftarrows\fi\ifnum #1=6\mapsto\fi\ifnum #1=7\iffalse\fi\fi\ifnum #1=8\twoheadrightarrow\fi\ifnum #1=9\rightharpoonup\fi\ifnum #1=0\rightharpoondown\fi}

%\newcommand{\la}[1]{\ifnum #1=1\leftarrow\fi\ifnum #1=2\Leftarrow\fi\ifnum #1=3\Lleftarrow\fi\ifnum #1=4\leftleftarrows\fi\ifnum #1=5\rightleftarrows\fi\ifnum #1=6\mapsfrom\ifnum #1=7\iffalse\fi\fi\ifnum #1=8\twoheadleftarrow\fi\ifnum #1=9\leftharpoonup\fi\ifnum #1=0\leftharpoondown\fi}

%\newcommand{\ua}[1]{\ifnum #1=1\uparrow\fi\ifnum #1=2\Uparrow\fi}
%\newcommand{\da}[1]{\ifnum #1=1\downarrow\fi\ifnum #1=2\Downarrow\fi}

%%% --Special Editor Config
\renewcommand{\ni}{\noindent}
\newcommand{\onum}[1]{\raisebox{.5pt}{\textcircled{\raisebox{-1pt} {#1}}}}

\newcommand{\claim}[1]{\underline{``{#1}":}}

\renewcommand{\l}{\left}
\renewcommand{\r}{\right}

%\newcommand{\casebrak}[2]{\left \{ \begin{array}{l} {#1}\\{#2} \end{array} \right.}
%\newcommand{\ttm}[4]{\l[\begin{array}{cc}{#1}&{#2}\\{#3}&{#4}\end{array}\r]} %two-by-two-matrix
%\newcommand{\tv}[2]{\l[\begin{array}{c}{#1}\\{#2}\end{array}\r]}

\def\dps{\displaystyle}

\let\italiccorrection=\/
\def\/{\ifmmode\expandafter\frac\else\italiccorrection\fi}


%%% --General Math Symbols
\def\bc{\because}
\def\tf{\therefore}

%%% --Frequently used OPERATORS shorthand
\newcommand{\INT}[2]{\int_{#1}^{#2}}
% \newcommand{\UPINT}{\bar\int}
% \newcommand{\UPINTRd}{\overline{\int_{\bb R ^d}}}
\newcommand{\SUM}[2]{\sum\limits_{#1}^{#2}}
\newcommand{\PROD}[2]{\prod\limits_{#1}^{#2}}
% \newcommand{\CUP}[2]{\bigcup\limits_{#1}^{#2}}
% \newcommand{\CAP}[2]{\bigcap\limits_{#1}^{#2}}
% \newcommand{\SUP}[1]{\sup\limits_{#1}}
% \newcommand{\INF}[1]{\inf\limits_{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\pd}[2]{\frac{\partial{#1}}{\partial{#2}}}
\def\tr{\text{tr}}

\renewcommand{\o}{\circ}
\newcommand{\x}{\times}
\newcommand{\ox}{\otimes}

%%% --Frequently used VARIABLES shorthand
\def\R{\ifmmode\mathbb R\fi}
\def\N{\ifmmode\mathbb N\fi}
\renewcommand{\O}{\mathcal{O}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\subsection*{Sec 1.1 Intro to Linear Systems}

Recall algebra, e.g. $x + 5 = 3$. Generalize to two variables:
\begin{equation*}
	\l\{{x+y= 5 \atop 3x-y = -1}\r..
\end{equation*}
Solving intuitively, $x = 1, y = 4$. The problem on Page 1:
\begin{equation}\label{sample3x3}
	\l\{
		\begin{array}{c}
			x+2y+3z = 39 \\
			x+3y+2z = 34 \\
			3x+2y+z = 26
		\end{array}
	\r..
\end{equation}
Answer is $x = 2.75, y = 4.25, z = 9.25$. 

\bigskip

Some systems are not (uniquely) solvable.
\begin{equation}\label{sample3x3nunique}
	\l\{
		\begin{array}{c}
			2x+4y+6z = 0 \\
			4x+5y+6z = 3 \\
			7x+8y+9z = 6
		\end{array}
	\r..
\end{equation}
\begin{equation}\label{sample3x3nsolvable}
	\l\{
		\begin{array}{c}
			x+2y+3z = 0 \\
			4x+5y+6z = 3 \\
			7x+8y+9z = 0
		\end{array}
	\r..
\end{equation}

\bigskip

Geometric interpretation: find points that lie on all three planes.

\subsubsection*{``Degrees of freedom" (from Sec 1.3)} 
\begin{equation*}
	\l\{
		\begin{array}{c}
			x+z = -7 \\
			x+3z = 3 \\
			x+5z = 13
		\end{array}
	\r. \qquad
	\l\{
		\begin{array}{c}
			x+y+z = 1 \\
			y+3z = 3 \\
		\end{array}
	\r. \qquad
	\l\{
		\begin{array}{c}
			x+y+4z = 1 \\
			x-y+z = 1 \\
			3x+y-z = 5 \\
			x+4y-6z = 0
		\end{array}
	\r.
\end{equation*}
First: $x = -12, z = 5$ but no constraint on $y$. 

\bigskip

Quick check, doesn't prove  solvability.


\subsubsection*{Geometric interpretation (from Sec 1.3)}

\begin{itemize}
	\item $ax+by+cz = 0$ defines a plane perpendicular to $(a, b, c)$ passing origin. Translate it to get $ax+by+cz = d$. 
	\item Intersection of planes, either unique or infinitely many solutions. (Houdini demo)
\end{itemize}

\subsubsection*{Solvability (from Sec 1.3)} 
Not solvable: contradiction after some reduction. See \eqref{sample3x3nsolvable}.\\
Infinite solutions: parametrization. See \eqref{sample3x3nunique}.

\subsection*{Sec 1.2 Matrices, Vectors, and Gauss-Jordan Elimination}
\begin{itemize}
	\item matrix dimension; row, column, index notation
	\item identity, zero, square, upper/lower triangular, symmetric matrices
	\item vector, vector spaces $\R^n$ (column vectors!)
	\item solve \eqref{sample3x3} using extended matrix
	\item Gaussian reduction: three operations
	\item RREF: definition, solve \eqref{sample3x3}, show \eqref{sample3x3nunique} is not full rank
\end{itemize}

\subsection*{Sec 1.3 On the Solutions of Linear Systems; Matrix Algebra}

\subsubsection*{Rank}
Matrices from \eqref{sample3x3} and \eqref{sample3x3nunique} have rank 3 and 2. Full rank matrix has identity in RREF.

\subsubsection*{Matrix Algebra}
\begin{itemize}
	\item from linear system to matrix-vector equation
	\item matrix addition, matrix-vector multiplication, matrix-matrix multiplication
	\item distribution law, commutative law etc.
	\item linear combination
	\item interpret matrix-vector multiplication as linear combination with columns
\end{itemize}

\subsection*{Sec 2.1 Linear Transformations}
\begin{equation}
	\l[
	{y_1 \atop y_2}
	\r]
	=
	\l[
	\begin{array}{cc}
		1 & 3\\
		2 & 5
	\end{array}
	\r]
	\l[
	{x_1 \atop x_2}
	\r]
\end{equation}
\begin{equation}
	\l[
	{y_1 \atop y_2}
	\r]
	=
	\l[
	\begin{array}{cc}
		1 & 3\\
		2 & 6
	\end{array}
	\r]
	\l[
	{x_1 \atop x_2}
	\r]
\end{equation}
\begin{equation}
	\l[
    \begin{array}{c}
        y_1\\
        y_2\\
        y_3
    \end{array}
	\r]
	=
	\l[
	\begin{array}{cc}
		1 & 3\\
		2 & 5\\
        -1 & 1
	\end{array}
	\r]
	\l[
	{x_1 \atop x_2}
	\r]
\end{equation}
\begin{itemize}
    \item linearity
    \item $Ae_i = T(e_i)$
    \item Finding the corresponding matrix
    \item \st{Markov chain: EXAMPLE 9 on p.5, distribution vectors and transition matrices} \red{(skipped)}
\end{itemize}

\subsection*{Sec 2.2 Linear Transformation in Geometry}
\begin{itemize}
    \item Geometric meaning of the four entries of a 2-by-2 matrix (scaling, shearing)
    \item \st{orthogonal projection, reflection in 2D} \red{take home}
    \item orthogonal projection, reflection w.r.t. a plane in 3D
    \item rotation in 2D
\end{itemize}

\subsection*{Sec 2.3 Matrix Products}
\begin{itemize}
    \item function composition
    \item non-commutativity
    \item \st{distributivity} \red{in homework}
    \item \st{block matrix multiplication} \red{skip}
\end{itemize}

\subsection*{Sec 2.4 The Inverse of a Linear Transformation}
\begin{itemize}
    \item injective, surjective, invertible functions and their composition
    \item invertible matrices: RREF, rank, row operations
    \item invertible linear systems: solvability
    \item $AA^{-1} = A^{-1}A = I$
    \item prove $(AB)^{-1} = B^{-1}A^{-1}$
    \item 2-by-2 matrix inverse formula
\end{itemize}

\subsection*{Sec 3.1 Image and Kernel of a Linear Transformation}
\begin{itemize}
    \item definition of image, kernel, and span
    \item finding image and kernel of matrix 
	$\l[
	\begin{array}{cc}
	    2 & 3 \\
	    6 & 9
	\end{array}
	\r]$,
	$\l[
	\begin{array}{cccc}
	    1&2&3&4 \\
	    0&1&2&3\\
	    0&0&0&1
	\end{array}
	\r]$
\end{itemize} 

\subsection*{Sec 3.2 Subspaces of $\R^n$; Bases and Linear Independence}
\subsubsection*{Subspaces}
\begin{itemize}
    \item subspace: closed under linear combination
    \item image and kernel are subspaces
    \item geometric interpretation
\end{itemize}
\subsubsection*{Bases}
\begin{itemize}
    \item linear independence; link to rank of a matrix
    \item nontrivial kernel = not linearly independent columns of a matrix, e.g. (page 129)
	$$A = \l[
	\begin{array}{ccc}
	    1&4&7\\
	    2&5&8\\
	    3&6&9
	\end{array}
	\r]$$
    \item basis = linearly independent spanning set
    \item find basis for subspaces $im(A), ker(A)$
    \item basis and unique representation
\end{itemize}

\subsection*{Sec 3.3 Dimension of a Subspace of $\R^n$}
\begin{itemize}
    \item dimension = \# of vectors in a basis
    \item e.g. $\R^n = span(\{e_1, \cdots, e_n\})$
    \item dimension is unique 
    \item dimension = maximal \# of linearly independent vectors = minimal \# of spanning vectors (Theorem 3.3.4 page 136)
    \item $v_1, \cdots, v_k$: linear independent $\ra2$ $Sv_1, \cdots, Sv_k$: linear independent where $S \in \R^{n\x n}$ is invertible
    \item example 1 page 136
    \item row reduction messes up the columns
    \item $rank(A) = rank(SA)$ for any $A\in \R^{m\x n}$ and $S \in GL(m, \R)$
    \item rank-nullity theorem: from RREF, \# pivot is rank, those columns without pivot is free variable (nullity), summing to $n$
\end{itemize}

\subsection*{Midterm 1 Review}
\begin{itemize}
    \item linear systems with a unique solution, infinitely many solutions, no solution (examples and such)
    \item orthogonal projection to a one dimensional subspace $\{\alpha n: \alpha \in \R\}$ 
    \item row reduction, RREF, solving linear system, extended matrix, inverting a matrix by applying row reduction simultaneously on $A$ and $I$
    \item meaning of rank (\# nonzero rows in RREF, dimension of image)
    \item links between rank, RREF, solvability of the linear system, invertibility of the matrix, linear independence of the columns, injectivity, surjectivity of the linear map; practice this with square matrices and rectangular matrices
    \item meaning of a subspace, linear combination, linear dependence, spanning set, basis, dimension
    \item how to find basis of the image and kernel of a given linear map $T(x) = Ax$
\end{itemize}


\subsection*{Sec 3.3 Dimension of a Subspace of $\R^n$ (Cont'd)}
\begin{itemize}
    \item rank nullity theorem: rank is number of pivots in RREF
\end{itemize}

\subsection*{Sec 3.4 Coordinates}
\begin{itemize}
    \item Notation of coordinates (page 149)
    \item example 2 (page 150)
    \item $[x]_{\mathcal B} = S^{-1}x$
    \item example 3 on page 151
    \item diagram on page 155
    \item $x = [x]_{\mathcal E}$, $[T]_{\mathcal B}$ using example 3
    \item rotation on a plane spanned by $(2, 1, -1)$ and $(0, 1, 1)$
    \item similar matrices (definition)
    \item diagonal $[T]_{\mathcal B}$ (page 157)
\end{itemize}

\subsection*{Sec 5.1 Orthogonal Projections and Orthonormal Bases}
\begin{itemize}
    \item orthonormal basis (definition)
    \item $S^{-1} = S^T$ for orthonormal basis
    \item orthogonal projection done by inner product with orthonormal basis, $\mbox{diag}(1, \cdots, 1, 0, \cdots, 0)Sx$
    \item theorem 5.1.6 (page 207)
    \item \st{orthogonal complement (definition)} \red{skip}
    \item \st{theorem 5.1.8 (page 208)} \red{skip}
    \item \st{Cauchy-Schwartz inequality} \red{skip}
\end{itemize}

\subsection*{Sec 5.2 Gram-Schmidt Process and QR Factorization}
\begin{itemize}
    \item Gram-Schmidt process: Example 1 (page 219)
    \item QR factorization: Example 2 (page 222)
\end{itemize}

\subsection*{Sec 5.3 Orthogonal Transformations and Orthogonal Matrices}
\begin{itemize}
    \item Definition 5.3.1 (page 225)
    \item $\|Ax\|^2 = x^TA^TAx = x^Tx \Leftrightarrow A^TA = I$, i.e. columns of $A$ are orthonormal! (Theorem 5.3.3 on page 227)
    \item Theorem 5.3.10 on page 232)
\end{itemize}

\subsection*{Sec 5.4 Least Squares and Data Fitting}
\begin{itemize}
    \item Definition 5.4.4 Least-squares solutions $\|b-Ax^\ast\|\leq \|b-Ax\| \forall x\in \R^m$.
    \item Thm 5.4.1, 5.4.2, 5.4.3, 5.4.5, or alternatively, multi-variable calculus to show $A^T(Ax^\ast -b) = 0$.
    \item Theorem 5.4.6, 5.4.7
    \item (maybe skip theorems) Example 4 (page 242), Example 6 (page 244) (linear regression)
\end{itemize}

\subsection*{Sec 6.1 Intro to Determinants}
\begin{itemize}
    \item 2 by 2 and 3 by 3
    \item $\det(A) = u\cdot (v\x w)$ for 3 by 3 matrices 
    \item $\det(A) = 0$ then $u, v, w$ are linear dependent
    \item Similarly, $\det(A) \neq 0$ then $A$ is invertible for 2 by 2 matrices 
\end{itemize}

\subsection*{Midterm 2 Review}
\begin{itemize}
    \item rank-nullity theorem (statement and usage)
    \item coordinates (computation with $S = [v_1 \cdots v_n]$, $S[x]_{\mathcal B} = x$), both for subspaces or $\R^n$
    \item orthonormal basis, both for subspaces or $\R^n$
    \item $S^{-1} = S^T$, \therefore $[x]_{\mathcal B}$ can be found by $u_i^Tx$
    \item orthogonal projection using orthonormal basis
    \item orthogonal complement (definition)
    \item Gram-Schmidt process, QR factorization
    \item orthogonal matrix (definition)
    \item Least-squares solutions: $Ax^\ast -b \in im(A)^\perp$, $A^TAx^\ast = A^Tb$
\end{itemize}


\subsection*{Sec 6.1 Intro to Determinants (Cont'd)}
\begin{itemize}
    \item $n$ by $n$ matrix (page 269)
    \item $S_n$ permutation groups, transitions, decomposition of permutations
    \item determinant of triangular matrix
    \item Sec.6.1. \#37 on page 276
\end{itemize}

\subsection*{Sec 6.2 Properties of the Determinant}
\begin{itemize}
    \item thm 6.2.1, transpose
    \item thm 6.2.2, linear in columns
    \item thm 6.2.3, row reductions
    \item thm 6.2.4, invertibility
    \item thm 6.2.6, product rule
    \item thm 6.2.8, inverse
    \item thm 6.2.10, Laplace expansion
\end{itemize}

\subsection*{Sec 6.3 Geometrical Interpretations of the Determinant; Cramer's Rule}
\begin{itemize}
    \item $Q \in \mathcal U(n) \ra2 \det(Q) = \pm1$
    \item rotation matrices, reflection
    \item thm 6.3.3 with QR factorization
    \item interpretation of thm 6.3.3. with 2D and 3D geometry
    \item thm 6.3.4
    \item thm 6.3.6 with QR factorization
    \item Jacobian (``expansion factor")
\end{itemize}

\subsection*{Sec 7.1 Diagonalization}
\begin{itemize}
    \item diagonalizable matrices: $A = PDP^{-1}$
    \item interpret $AP = PD$ with Eigenvectors, eigenvalues, and eigenbases
    \item Example 4 (importance to determine the field)
    \item Example 5
    \item Example 7
\end{itemize}

\subsection*{Sec 7.2 Finding the Eigenvalues of a Matrix}
\begin{itemize}
    \item characteristic equation and its zeros
    \item Example 2 (then thm.7.2.2)
    \item characteristic polynomial (thm 7.2.5) $f_A(\lambda)$
    \item def 7.2.6 (algebraic multiplicity of an eigenvalue)
    \item Newton's method to find rational roots
    \item polynomial division
    \item decomposition of polynomials
\end{itemize}
Note: maybe mention the definition of trace and show its cyclic property.
\red{\\May 25, 2019}

\subsection*{Sec 7.3 Finding the Eigenvectors of a Matrix}
\begin{itemize}
    \item def 7.3.1 $E_\lambda = ker(A-\lambda I)$, Example 1, 2
    \item geometric multiplicity
    \item thm 7.3.3 (eigenbases and geometric multiplicities), 7.3.4 (all eigenvalues are distinct)
    \item thm 7.3.5 (similar matrices)
    \item thm 7.3.6 (geometric multiplicity $\leq$ algebraic multiplicity)
\end{itemize}

\subsection*{Sec 7.4 More on Dynaimcal Systems}
\begin{itemize}
    \item Example 1 (page 347)
    \item thm 7.4.1 (equilibria)
    \item Example 2 (page 351)
\end{itemize}





% this is Sec 6.3
    % \item $\det(A)$ is the area of the parallelogram/parallelepiped


\end{document}
