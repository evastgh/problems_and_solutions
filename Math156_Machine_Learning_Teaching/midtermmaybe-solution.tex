\documentclass[12pt,a4paper]{article}
	%[fleqn] %%% --to make all equation left-algned--

% \usepackage[utf8]{inputenc}
% \DeclareUnicodeCharacter{1D12A}{\doublesharp}
% \DeclareUnicodeCharacter{2693}{\anchor}
% \usepackage{dingbat}
% \DeclareRobustCommand\dash\unskip\nobreak\thinspace{\textemdash\allowbreak\thinspace\ignorespaces}
\usepackage[top=1.1in, bottom=.9in, left=.9in, right=.9in]{geometry}
%\usepackage{fullpage}

\usepackage{fancyhdr}\pagestyle{fancy}\rhead{May 4, 2019}\lhead{Math 156 -- Midterm}

\usepackage{nicefrac, tabularx}

\usepackage{amsmath,amssymb,amsthm,amsfonts,microtype,stmaryrd}
	%{mathtools,wasysym,yhmath}

\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\fgreen}[1]{\textcolor{ForestGreen}{#1}}


\usepackage{enumitem}

\usepackage{mdframed}
	%\newtheorem{mdexample}{Example}
	\definecolor{warmgreen}{rgb}{0.8,0.9,0.85}
	% --Example:
	% \begin{center}
	% \begin{minipage}{0.7\textwidth}
	% \begin{mdframed}[backgroundcolor=warmgreen, 
	% skipabove=4pt,skipbelow=4pt,hidealllines=true, 
	% topline=false,leftline=false,middlelinewidth=10pt, 
	% roundcorner=10pt] 
	%%%% --CONTENTS-- %%%%
	% \end{mdframed}\end{minipage}\end{center}	

%\usepackage{graphicx} \graphicspath{ {/path/} }
	% --Example:
	% \includegraphics[scale=0.5]{picture name}
%\usepackage{caption} %%% --some awful package to make caption...

%\usepackage{hyperref}\hypersetup{linktocpage,colorlinks}\hypersetup{citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}

%%% --Text Fonts
%\usepackage{times} %%% --Times New Roman for LaTeX
%\usepackage{fontspec}\setmainfont{Times New Roman} %%% --Times New Roman; XeLaTeX only

%%% --Math Fonts
\renewcommand{\v}[1]{\ifmmode\mathbf{#1}\fi}
%\renewcommand{\mbf}[1]{\mathbf{#1}} %%% --vector
%\newcommand{\ca}[1]{\mathcal{#1}} %%% --"bigO"
%\newcommand{\bb}[1]{\mathbb{#1}} %%% --"Natural, Real numbers"
%\newcommand{\rom}[1]{\romannumeral{#1}} %%% --Roman numbers

%%% --Quick Arrows
\newcommand{\ra}[1]{\ifnum #1=1\rightarrow\fi\ifnum #1=2\Rightarrow\fi\ifnum #1=3\Rrightarrow\fi\ifnum #1=4\rightrightarrows\fi\ifnum #1=5\rightleftarrows\fi\ifnum #1=6\mapsto\fi\ifnum #1=7\iffalse\fi\fi\ifnum #1=8\twoheadrightarrow\fi\ifnum #1=9\rightharpoonup\fi\ifnum #1=0\rightharpoondown\fi}

%\newcommand{\la}[1]{\ifnum #1=1\leftarrow\fi\ifnum #1=2\Leftarrow\fi\ifnum #1=3\Lleftarrow\fi\ifnum #1=4\leftleftarrows\fi\ifnum #1=5\rightleftarrows\fi\ifnum #1=6\mapsfrom\ifnum #1=7\iffalse\fi\fi\ifnum #1=8\twoheadleftarrow\fi\ifnum #1=9\leftharpoonup\fi\ifnum #1=0\leftharpoondown\fi}

%\newcommand{\ua}[1]{\ifnum #1=1\uparrow\fi\ifnum #1=2\Uparrow\fi}
%\newcommand{\da}[1]{\ifnum #1=1\downarrow\fi\ifnum #1=2\Downarrow\fi}

%%% --Special Editor Config
\renewcommand{\ni}{\noindent}
\newcommand{\onum}[1]{\raisebox{.5pt}{\textcircled{\raisebox{-1pt} {#1}}}}

\newcommand{\claim}[1]{\underline{``{#1}":}}

\renewcommand{\l}{\left}
\renewcommand{\r}{\right}

%\newcommand{\casebrak}[2]{\left \{ \begin{array}{l} {#1}\\{#2} \end{array} \right.}
%\newcommand{\ttm}[4]{\l[\begin{array}{cc}{#1}&{#2}\\{#3}&{#4}\end{array}\r]} %two-by-two-matrix
%\newcommand{\tv}[2]{\l[\begin{array}{c}{#1}\\{#2}\end{array}\r]}

\def\dps{\displaystyle}

\let\italiccorrection=\/
\def\/{\ifmmode\expandafter\frac\else\italiccorrection\fi}


%%% --General Math Symbols
\def\bc{\because}
\def\tf{\therefore}

%%% --Frequently used OPERATORS shorthand
\newcommand{\INT}[2]{\int_{#1}^{#2}}
% \newcommand{\UPINT}{\bar\int}
% \newcommand{\UPINTRd}{\overline{\int_{\bb R ^d}}}
\newcommand{\SUM}[2]{\sum\limits_{#1}^{#2}}
\newcommand{\PROD}[2]{\prod\limits_{#1}^{#2}}
% \newcommand{\CUP}[2]{\bigcup\limits_{#1}^{#2}}
% \newcommand{\CAP}[2]{\bigcap\limits_{#1}^{#2}}
% \newcommand{\SUP}[1]{\sup\limits_{#1}}
% \newcommand{\INF}[1]{\inf\limits_{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\pd}[2]{\frac{\partial{#1}}{\partial{#2}}}
\def\tr{\text{tr}}

\renewcommand{\o}{\circ}
\newcommand{\x}{\times}
\newcommand{\ox}{\otimes}

%%% --Frequently used VARIABLES shorthand
\def\R{\ifmmode\mathbb R\fi}
\def\N{\ifmmode\mathbb N\fi}
\renewcommand{\O}{\mathcal{O}}


\newcommand{\solution}[1]{\gray{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\noindent\textbf{Instructions:}
\begin{itemize}
	\item You have until Tuesday May 5th 2:59pm PST to finish and upload your answers to CCLE. For the complete regulations, see the announcement on CCLE.
    \item On the first page of your submission, write down your full name, University ID, and the following academic integrity statement, then sign and date. 
\begin{center}
    \parbox{0.8\textwidth}{
\textsf{I agree to the UCLA Student Code of Conduct for academic integrity. I agree to use NO resource other than lecture notes and the textbook for this exam. I agree to communicate with NO ONE regarding this exam. Violations of this code will result in immediate FAILURE of this course. }}
\end{center}
	\item Use blank sheets of paper to write down answers with requested supporting work.
\end{itemize}

\newpage
\subsubsection*{Problem 1}
Given a data set 
\begin{equation*}
\mathcal D = \{ (\v x^{(i)}, t^{(i)}) \}_{i=1}^N, \hskip10pt \v x^{(i)} \in \R^D, t^{(i)} \in \R.
\end{equation*}
Fixing an integer $M \in \N$ and a basis function $\boldsymbol\phi: \R^D \ra1 \R^M$. The regression problem is about finding the best parameter $\v w \in \R^M$ so
\begin{equation*}
t^{(i)} = \v w^T \boldsymbol\phi(\v x^{(i)}) + \boldsymbol \epsilon^{(i)}
\end{equation*}
where $\epsilon^{(i)} \sim \mathcal N(0, \beta^{-1})$ are independent identical (unbiased) Gaussian noise. 
%
\begin{enumerate}[label=(\alph*)]
%
\item Write down a formula for the likelihood function $p(\mathcal D \vert \v w, \beta)$. (10 points) 
\solution{
\begin{equation*}
p(\mathcal D \vert \v w, \beta) = \prod_{i=1}^N \mathcal N(t^{(i)} \vert \v w^T\boldsymbol\phi(\v x^{(i)}), \beta^{-1}).
\end{equation*}}
%
\item Show that the maximum likelihood solution 
\begin{equation*}
\v w^\ast_\beta = \argmax_{\v w} p(\mathcal D \vert \v w, \beta)
\end{equation*} for any value of $\beta > 0$ is the same as the least square solution 
\begin{equation*}
\overline {\v w} = \argmin_{\v w} \SUM{i=1}N \frac12 \l|t^{(i)} - y(x^{(i)}, \v w)\r|^2. \hskip20pt\text{(10 points)}
\end{equation*}
\solution{
The least square error function $\frac12 \l|t_i - y(x^{(i)}, \v w)\r|^2$ is the same as the negative log-likelihood divided by $\beta>0$ (up to a constant).}
%
\item Fixing the model complexity $M \in \N$, give three examples of the basis function $\boldsymbol\phi(\v x)$. (10 points)
\solution{\\Linear $(1, \v x)$, spline functions, Gaussian, sigmoidal, etc. }
\end{enumerate}


\newpage
\subsubsection*{Problem 2}
Suppose a data set $\mathcal D = \{ (\v x^{(i)}, t^{(i)})\}_{i=1}^N$ is given. $\v x^{(i)} \in \R^D, t^{(i)} \in \R$ for $i = 1, \cdots, N$. 
%
\begin{enumerate}[label=(\alph*)]
%
\item Show that the optimal solution $\v w^\ast = \argmin J(\v w)$ for a regularized sum-of-squares error function
\begin{equation*}
J(\v w) = \/12 \SUM{i=1}N \l(t^{(i)} - \v w^T \boldsymbol\phi(\v x^{(i)}) \r)^2 + \/\lambda2\|\v w\|^2,
\end{equation*}
where $\lambda > 0$, is a linear combination of the vectors $\{\boldsymbol \phi(\v x^{(i)}) \}_{i=1}^N$. In other words, show that 
\begin{equation*}
\v w^\ast = \SUM{i=1}N a^{(i)} \boldsymbol\phi(\v x^{(i)})
\end{equation*}
for some scalars $a^{(i)} \in \R, i = 1, \cdots, N$. (10 points) 
\solution{\\ From the optimal condition
\begin{align*}
0 & = \nabla J(\v w^\ast) = \SUM{i=1}N (t^{(i)} - (\v w^\ast)^T \boldsymbol\phi(\v x^{(i)})) (-\boldsymbol\phi(\v x^{(i)})) + \lambda \v w^\ast \\
\mathbf{ w}^\ast &= \/1\lambda \SUM{i=1}N (t^{(i)} - (\v w^\ast)^T \boldsymbol\phi(\v x^{(i)}))\boldsymbol\phi(\v x^{(i)}) = \SUM{i=1}N a^{(i)} \boldsymbol\phi(\v x^{(i)}) \\
a^{(i)} &= \/1\lambda (t^{(i)} - (\v w^\ast)^T \boldsymbol\phi(\v x^{(i)})).
\end{align*}
(I let go of missing negative sign for this problem.)
Alternatively, one can show that 
\begin{align*}
\mathbf w^\ast &= (\v K + \lambda \v I)^{-1}\boldsymbol\Phi^T \v t \\
&= \boldsymbol\Phi^T  (\v K + \lambda \v I)^{-1} \v t \\ 
&= \SUM{i=1}N \boldsymbol\phi(\v x^{(i)})^T \text{($i$-th entry of $(\v K + \lambda \v I)^{-1}\v t$)} \\
& =\boldsymbol\Phi^T \v a.
\end{align*}
This requires showing that $\boldsymbol\Phi^T$ commutes with the matrix $(\v K + \lambda \v I)^{-1}$ but will solve problem 2c together. 
}
%
\item We define the Gram matrix 
\begin{equation*}
\v K = [\ K_{ij}\ ] = [\ \boldsymbol\phi(\v x^{(i)})^T \boldsymbol\phi(\v x^{(j)}) \ ] \in \R^{N \x N}.
\end{equation*}
Show that $\v K$ is symmetric semi-positive definite. (A matrix $\v A \in \R^{N\x N}$ is symmetric semi-positive definite if and only if $\v x^T \v A \v x \geq 0$ for all vector $\v x \in \R^N$.) (10 points) 
%hence $\v K + \lambda \v I$ is always invertible for any $\lambda > 0$ (5 points). \\
\solution{\\
Take any vector $\v y \in \R^N$, 
\begin{align*}
\mathbf y^T \v K \v y &= \SUM{i=1}N\SUM{j=1}N y_i \boldsymbol\phi(\v x^{(i)})\boldsymbol\phi(\v x^{(j)}) y_j \\
&= \l(\SUM{i=1}N y_i \boldsymbol\phi(\v x^{(i)}) \r)^2 \geq 0.
\end{align*}
}
%
\item Show that the coefficients from part (a) satisfy
\begin{equation*}
(\v K + \lambda \v I_N) \l[
\begin{array}{c}
a^{(1)} \\
a^{(2)} \\
\vdots \\
a^{(N)}
\end{array}
\r] =  
\l[
\begin{array}{c}
t^{(1)} \\
t^{(2)} \\
\vdots \\
t^{(N)}
\end{array}
\r]. \hskip20pt \text{(10 points)}
\end{equation*}
\solution{
Since $\v w^\ast = \boldsymbol\Phi \v a$, where 
\begin{equation*}
\boldsymbol\Phi = \l[\begin{array}{c}
\boldsymbol\phi(\v x^{(1)})^T \\
\boldsymbol\phi(\v x^{(2)})^T \\
\vdots \\
\boldsymbol\phi(\v x^{(N)})^T
\end{array}
\r] \in \R^{N \x M},
\end{equation*}
we have
\begin{align*}
\lambda a^{(i)} & = - (t^{(i)} - (\v w^\ast)^T \boldsymbol\phi(\v x^{(i)})) \\
& = -(t^{(i)} - \v a^T \boldsymbol\Phi^T \boldsymbol\phi(\v x^{(i)})) \\
\lambda \v I_N \v a & = \v t - \boldsymbol\Phi \boldsymbol\Phi^T \v a = \v t - \v K \v a.
\end{align*}
}
\end{enumerate}







\newpage
\subsubsection*{Problem 3}
Consider the two-class classification problem. Denote the data set $\mathcal D = \{(\v x^{(i)}, t^{(i)}) \}_{i=1}^N$ where 
\begin{equation*}
t^{(i)} = \l\{{1, \;\; i \in C_{[1]} \atop 0, \;\; i \in C_{[2]}} \r.
\end{equation*}
is the target variable encoding the class membership. 
%
\begin{enumerate}[label=(\alph*)]
%
\item Suppose $p(\v x \vert C_{[1]}) \sim \mathcal N(\boldsymbol\mu_{[1]}, \boldsymbol\Sigma)$ and $p(\v x \vert C_{[2]}) \sim \mathcal N(\boldsymbol\mu_{[2]}, \boldsymbol\Sigma)$, that is, data from two classes scatter around different class-specific mean but share the same covariance matrix. Denote $p(C_{[1]}) = \pi$, hence $p(C_{[2]}) = 1-\pi$. The likelihood function is given by 
\begin{equation*}
\begin{split}
p(\mathcal D \vert \pi, \boldsymbol\mu_{[1]}, \boldsymbol\mu_{[2]}, \boldsymbol\Sigma) 
% & = \l(\prod_{i\in C_{[1]}} p(C_{[1]}) p(\v x^{(i)} \vert C_{[1]}) \r)
%\cdot \l(\prod_{i\in C_{[2]}}  p(C_{[2]}) p(\v x^{(i)} \vert C_{[2]}) \r)\\
& = \l(\prod_{i\in C_{[1]}} \pi \mathcal N(\v x^{(i)} \vert \boldsymbol\mu_{[1]}, \boldsymbol\Sigma)  \r) 
\cdot \l(\prod_{i\in C_{[2]}} (1-\pi)  \mathcal N(\v x^{(i)} \vert \boldsymbol\mu_{[2]}, \boldsymbol\Sigma)\r)
\end{split}
\end{equation*}
Show that the maximum likelihood estimate of the class probability $\pi$ is given by the fraction of data points in $C_{[1]}$, i.e.
\begin{equation*}
\argmax_{\pi} p(\mathcal D \vert \pi, \boldsymbol\mu_{[1]}, \boldsymbol\mu_{[2]}, \boldsymbol\Sigma) = \frac{ \#\{i : i \in C_{[1]} \} 
}N. \hskip20pt \text{(10 points)}
\end{equation*}
\solution{
Log-likelihood function boils down to
\begin{equation*}
E(\pi) = \SUM{i\in C_{[1]}}{} \log\pi + \SUM{i\in C_{[2]}}{} \log(1-\pi) + \text{(const w.r.t. $\pi$)}.
\end{equation*}
The first optimality condition gives
\begin{align*}
E'(\pi) &= \SUM{i\in C_{[1]}}{} \frac1\pi - \SUM{i\in C_{[2]}}{} \frac1{1-\pi} = 0 \\
\pi(1-\pi) E'(\pi) &= N_1 (1-\pi) - N_2 \pi = N_1 - (N_1 + N_2) \pi = 0 \\
\pi & = \frac{N_1}{N_1 + N_2}.
\end{align*}
Here $N_1 = \#\{i : i \in C_{[1]} \}$, $N_2 = \#\{i : i \in C_{[2]} \}$.
}
%
\item The \textit{logistic sigmoid} function is defined by
\begin{equation*}
\sigma(b) = \frac1{1+e^{-b}}.
\end{equation*}
Show that (i) $\sigma(-b) = 1-\sigma(b)$, (ii) $\sigma$ is a monotonically increasing function, and (iii) $\sigma$ maps all of $\R$ onto the interval $(0, 1)$. (15 points) \\
\solution{
(i) $\displaystyle 1-\sigma(b) = 1-\frac1{1+e^{-b}} = \frac{e^{-b}}{1+e^{-b}} = \frac{1}{e^{b} + 1} = \sigma(-b)$. \\
(ii) $\displaystyle \sigma'(b) = \frac{-(-e^{-b})}{(1+e^{-b})^2} > 0$ for any $b \in \R$. \\
(iii) $\displaystyle\lim_{b\ra1-\infty}\sigma(b) = 0, \lim_{b\ra1+\infty} \sigma(b) = 1$. Since $\sigma$ is monotonically increasing, any input between $\pm\infty$ must fall in the interval $(0, 1)$.
}
%
\item In an approach different from part (a), we suppose that $p(C_{[1]} \vert \v x) = \sigma(\v w^T \boldsymbol\phi(\v x))$ where $\v w \in \R^M$ is a coefficient vector to be trained. According to part (b), $p(C_{[2]} \vert \v x) = \sigma(-\v w^T \boldsymbol\phi(\v x))$. The likelihood function is then given by this different formula,
\begin{equation*}
p(\mathcal D \vert \v w) = \l(\prod_{i\in C_{[1]}} \sigma(\v w^T \boldsymbol\phi(\v x^{(i)}))\r) \cdot \l(\prod_{i\in C_{[2]}} \sigma( - \v w^T \boldsymbol\phi(\v x^{(i)}))\r).
\end{equation*}
Define the error function $E(\v w) = -\log p(\mathcal D \vert \v w)$ to be the negative logarithm of the likelihood. Compute $\nabla E(\v w)$ and explain why the maximum likelihood estimate $\nabla E(\v w^\ast) = 0$ doesn't have an analytical solution. (15 points) 
\solution{
\begin{align*}
\frac{d}{dt}\log\sigma(t) &= \frac{\sigma'(t)}{\sigma(t)} = \frac{(1+e^{-t})e^{-t}}{(1+e^{-t})^2} = 1 - \sigma(t) = \sigma(-t)\\
E(\v w) & = -\l(\sum_{i\in C_{[1]}} \log\sigma(\v w^T \boldsymbol\phi(\v x^{(i)}))\r) - \l(\sum_{i\in C_{[2]}} \log \sigma( - \v w^T \boldsymbol\phi(\v x^{(i)}))\r) \\
\nabla E(\v w) & = -\l(\sum_{i\in C_{[1]}} \sigma(-\v w^T \boldsymbol\phi(\v x^{(i)}))\boldsymbol\phi(\v x^{(i)})\r) - \l(\sum_{i\in C_{[2]}} \sigma(\v w^T \boldsymbol\phi(\v x^{(i)}))(-\boldsymbol\phi(\v x^{(i)}))\r) \\
& = \l(\sum_{i\in C_{[1]}} (\sigma(\v w^T \boldsymbol\phi(\v x^{(i)}))-1)\boldsymbol\phi(\v x^{(i)})\r) + \l(\sum_{i\in C_{[2]}} \sigma(\v w^T \boldsymbol\phi(\v x^{(i)}))\boldsymbol\phi(\v x^{(i)})\r) \\
& = \SUM{i=1}N (\sigma(\v w^T \boldsymbol\phi(\v x^{(i)})) - t^{(i)}) \boldsymbol\phi(\v x^{(i)})
\end{align*}
Dependence on $\sigma$ makes the expression nonlinear and therefore have no analytical solution.
}
%
\item \textbf{(Bonus)} Provide a strategy to compute the optimal solution $\v w^\ast$ numerically. (up to 10 points) \\
\solution{Any clearly stated suggestion for Newton's method or Gradient Descend will receive up to 10 points.}
\end{enumerate}
\end{document}








Write down the definition for a \textit{kernel} function $k: \R^D \x \R^D \ra1 \R$. (5 points) \\
\\
(b) Show that 
\begin{equation*}
k(\v x, \v x') = e^{-\/12 \| \v x - \v x'\|^2}
\end{equation*}
defines a kernel function. You can assume if $k_1(\v x, \v x')$ is a kernel, then $k_2(\v x, \v x') := \exp(k_1(\v x, \v x'))$ is a kernel, too. (10 points) \\
\\




(Non-uniqueness of weight vector and bias) Suppose the following linear discriminant 
\begin{equation*}
y(\v x, \v w^\ast, w_0^\ast) = (\v w^\ast)^T \v x + w_0^\ast, 
\end{equation*}
\begin{equation*}
\v x \in \l\{ {
C_{[1]}, \text{ if } y(\v x, \v w^\ast, w_0^\ast) > 0 
\atop
C_{[2]}, \text{ if } y(\v x, \v w^\ast, w_0^\ast) < 0
} \r.
\end{equation*}
separates the two classes on the data set. Show that for any $\alpha >0$, $(\alpha \v w^\ast, \alpha w_0^\ast)$ separates the data set as well. (5 points)


