\documentclass[12pt,a4paper]{article}
	%[fleqn] %%% --to make all equation left-algned--

% \usepackage[utf8]{inputenc}
% \DeclareUnicodeCharacter{1D12A}{\doublesharp}
% \DeclareUnicodeCharacter{2693}{\anchor}
% \usepackage{dingbat}
% \DeclareRobustCommand\dash\unskip\nobreak\thinspace{\textemdash\allowbreak\thinspace\ignorespaces}
\usepackage[top=2in, bottom=1in, left=1in, right=1in]{geometry}
%\usepackage{fullpage}

\usepackage{fancyhdr}\pagestyle{fancy}\rhead{Stephanie Wang}\lhead{EE236C homework 5}
\usepackage{nicefrac}

\usepackage{amsmath,amssymb,amsthm,amsfonts,microtype,stmaryrd}
	%{mathtools,wasysym,yhmath}

\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\fgreen}[1]{\textcolor{ForestGreen}{#1}}

\usepackage{mdframed}
	%\newtheorem{mdexample}{Example}
	\definecolor{warmgreen}{rgb}{0.8,0.9,0.85}
	% --Example:
	% \begin{center}
	% \begin{minipage}{0.7\textwidth}
	% \begin{mdframed}[backgroundcolor=warmgreen, 
	% skipabove=4pt,skipbelow=4pt,hidealllines=true, 
	% topline=false,leftline=false,middlelinewidth=10pt, 
	% roundcorner=10pt] 
	%%%% --CONTENTS-- %%%%
	% \end{mdframed}\end{minipage}\end{center}	

\usepackage{graphicx} \graphicspath{{}}
	% --Example:
	% \includegraphics[scale=0.5]{picture name}
%\usepackage{caption} %%% --some awful package to make caption...

\usepackage{hyperref}\hypersetup{linktocpage,colorlinks}\hypersetup{citecolor=black,filecolor=black,linkcolor=black,urlcolor=blue,breaklinks=true}

%%% --Text Fonts
%\usepackage{times} %%% --Times New Roman for LaTeX
%\usepackage{fontspec}\setmainfont{Times New Roman} %%% --Times New Roman; XeLaTeX only

%%% --Math Fonts
\renewcommand{\v}[1]{\ifmmode\mathbf{#1}\fi}
%\renewcommand{\mbf}[1]{\mathbf{#1}} %%% --vector
%\newcommand{\ca}[1]{\mathcal{#1}} %%% --"bigO"
%\newcommand{\bb}[1]{\mathbb{#1}} %%% --"Natural, Real numbers"
%\newcommand{\rom}[1]{\romannumeral{#1}} %%% --Roman numbers

%%% --Quick Arrows
\newcommand{\ra}[1]{\ifnum #1=1\rightarrow\fi\ifnum #1=2\Rightarrow\fi\ifnum #1=3\Rrightarrow\fi\ifnum #1=4\rightrightarrows\fi\ifnum #1=5\rightleftarrows\fi\ifnum #1=6\mapsto\fi\ifnum #1=7\iffalse\fi\fi\ifnum #1=8\twoheadrightarrow\fi\ifnum #1=9\rightharpoonup\fi\ifnum #1=0\rightharpoondown\fi}

%\newcommand{\la}[1]{\ifnum #1=1\leftarrow\fi\ifnum #1=2\Leftarrow\fi\ifnum #1=3\Lleftarrow\fi\ifnum #1=4\leftleftarrows\fi\ifnum #1=5\rightleftarrows\fi\ifnum #1=6\mapsfrom\ifnum #1=7\iffalse\fi\fi\ifnum #1=8\twoheadleftarrow\fi\ifnum #1=9\leftharpoonup\fi\ifnum #1=0\leftharpoondown\fi}

%\newcommand{\ua}[1]{\ifnum #1=1\uparrow\fi\ifnum #1=2\Uparrow\fi}
%\newcommand{\da}[1]{\ifnum #1=1\downarrow\fi\ifnum #1=2\Downarrow\fi}

%%% --Special Editor Config
\renewcommand{\ni}{\noindent}
\newcommand{\onum}[1]{\raisebox{.5pt}{\textcircled{\raisebox{-1pt} {#1}}}}

\newcommand{\claim}[1]{\underline{``{#1}":}}

\renewcommand{\l}{\left}\renewcommand{\r}{\right}

\newcommand{\casebrak}[4]{\left \{ \begin{array}{ll} {#1},&{#2}\\{#3},&{#4} \end{array} \right.}
\newcommand{\ttm}[4]{\l[\begin{array}{cc}{#1}&{#2}\\{#3}&{#4}\end{array}\r]} %two-by-two-matrix
\newcommand{\tv}[2]{\l[\begin{array}{c}{#1}\\{#2}\end{array}\r]}

\def\dps{\displaystyle}

\let\italiccorrection=\/
\def\/{\ifmmode\expandafter\frac\else\italiccorrection\fi}


%%% --General Math Symbols
\def\bc{\because}
\def\tf{\therefore}

%%% --Frequently used OPERATORS shorthand
\newcommand{\INT}[2]{\int_{#1}^{#2}}
% \newcommand{\UPINT}{\bar\int}
% \newcommand{\UPINTRd}{\overline{\int_{\bb R ^d}}}
\newcommand{\SUM}[2]{\sum\limits_{#1}^{#2}}
\newcommand{\PROD}[2]{\prod\limits_{#1}^{#2}}
\newcommand{\CUP}[2]{\bigcup\limits_{#1}^{#2}}
\newcommand{\CAP}[2]{\bigcap\limits_{#1}^{#2}}
% \newcommand{\SUP}[1]{\sup\limits_{#1}}
% \newcommand{\INF}[1]{\inf\limits_{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\pd}[2]{\frac{\partial{#1}}{\partial{#2}}}
\def\tr{\text{tr}}

\renewcommand{\o}{\circ}
\newcommand{\x}{\times}
\newcommand{\ox}{\otimes}

\newcommand\ie{{\it i.e. }}
\newcommand\wrt{{w.r.t. }}
\newcommand\dom{\mathbf{dom\:}}

%%% --Frequently used VARIABLES shorthand
\def\R{\ifmmode\mathbb R\fi}
\def\N{\ifmmode\mathbb N\fi}
\renewcommand{\O}{\mathcal{O}}

\newcommand{\dt}{\Delta t}
\def\vA{\mathbf{A}}
\def\vB{\mathbf{B}}\def\cB{\mathcal{B}}
\def\vC{\mathbf{C}}
\def\vD{\mathbf{D}}
\def\vE{\mathbf{E}}
\def\vF{\mathbf{F}}\def\tvF{\tilde{\mathbf{F}}}
\def\vG{\mathbf{G}}
\def\vH{\mathbf{H}}
\def\vI{\mathbf{I}}\def\cI{\mathcal{I}}
\def\vJ{\mathbf{J}}
\def\vK{\mathbf{K}}
\def\vL{\mathbf{L}}\def\cL{\mathcal{L}}
\def\vM{\mathbf{M}}
\def\vN{\mathbf{N}}\def\cN{\mathcal{N}}
\def\vO{\mathbf{O}}
\def\vP{\mathbf{P}}
\def\vQ{\mathbf{Q}}
\def\vR{\mathbf{R}}
\def\vS{\mathbf{S}}
\def\vT{\mathbf{T}}
\def\vU{\mathbf{U}}
\def\vV{\mathbf{V}}
\def\vW{\mathbf{W}}
\def\vX{\mathbf{X}}
\def\vY{\mathbf{Y}}
\def\vZ{\mathbf{Z}}

\def\va{\mathbf{a}}
\def\vb{\mathbf{b}}
\def\vc{\mathbf{c}}
\def\vd{\mathbf{d}}
\def\ve{\mathbf{e}}
\def\vf{\mathbf{f}}
\def\vg{\mathbf{g}}
\def\vh{\mathbf{h}}
\def\vi{\mathbf{i}}
\def\vj{\mathbf{j}}
\def\vk{\mathbf{k}}
\def\vl{\mathbf{l}}
\def\vm{\mathbf{m}}
\def\vn{\mathbf{n}}
\def\vo{\mathbf{o}}
\def\vp{\mathbf{p}}
\def\vq{\mathbf{q}}
\def\vr{\mathbf{r}}
\def\vs{\mathbf{s}}
\def\vt{\mathbf{t}}
\def\vu{\mathbf{u}}
\def\vv{\mathbf{v}}\def\tvv{\tilde{\mathbf{v}}}
\def\vw{\mathbf{w}}
\def\vx{\mathbf{x}}\def\tvx{\tilde{\mathbf{x}}}
\def\vy{\mathbf{y}}
\def\vz{\mathbf{z}}

\def\diag{\mbox{\textbf{diag}}}

\newcommand{\kp}[1]{{k+{#1}}}
\newcommand{\km}[1]{{k-{#1}}}
\newcommand{\np}[1]{{n+{#1}}}
\newcommand{\nm}[1]{{n-{#1}}}

%%% --Numerical analysis related
%\newcommand{\nxt}{^{n+1}}
%\newcommand{\pvs}{^{n-1}}
%\newcommand{\hfnxt}{^{n+\frac12}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\subsubsection*{Problem 1.}
(a) First do the problem in one dimension. Let $f(x) = |x|, d(y) = \mu-\mu\sqrt{1-y^2}$. 
\begin{align*}
  f^\ast(y) & = \delta_{[-1, 1]}(y) \\
  g(y)      & = f^\ast(y) + \mu - \mu\sqrt{1-y^2} \\
  g^\ast(x) % & = \sup_y xy - g(y) \\
            & = \sup_{-1\leq y\leq 1} xy - \mu + \mu \sqrt{1-y^2}.
            % & = \sqrt{x^2+\mu^2} - \mu
\end{align*}
Differentiate the last expression with regard to $y$, we get 
$x-\frac{\mu y}{\sqrt{1-y^2}} = 0$. From here we deduce the optimizer $y^\ast$ satisfies that $sgn(x) = sgn(y^\ast)$. Solve the quadratic equation
$$(1-y^2)x^2 = \mu^2y^2,$$
one get $y^\ast = \frac{x}{\sqrt{x^2+\mu^2}}$; check that $y^\ast \in [-1, 1]$, indeed. The optimal value is 
$$xy^\ast - \mu + \mu\sqrt{1-{y^\ast}^2} = \frac{x^2}{\sqrt{x^2+\mu^2}} -\mu + \mu \sqrt{\frac{\mu^2}{x^2+\mu^2}} = \sqrt{x^2+\mu^2}-\mu. $$
Taking the result above, we can now conclude for $n$-dimensional case, $f(x) = \|x\|_1, d(y) = \mu\SUM{i=1}n (1-\sqrt{1-y_i^2})$, and
$$(f^\ast + d)^\ast(x) = \SUM{i=1}n (\sqrt{x_i^2+\mu^2} - \mu).$$

\noindent(b) First observe that 
$$f(x) = \max_{i=1,\cdots,n} x_i = \max_{\mathbf1^Tp = 1, p\succeq0} p^Tx.$$
We know the Legendre transform of a supporting function is the indicator functions. Moving forward,
\begin{align*}
  g(y) &= \delta_{\mathbf1^Ty=1, y\succeq0}(y) + \mu\l(\SUM{i=1}n y_i\log y_i + \log n\r) \\
  g^\ast (x) &= \sup_{\mathbf1^Ty=1, y\succeq0} x^Ty - \mu\SUM{i=1}n y_i\log y_i - \mu\log n
\end{align*}
Write down the Lagrangian for this constrained optimization,
$$\mathcal L(y, \lambda, \nu) = x^Ty - \mu\SUM{i=1}n y_i\log y_i - \mu\log n - \lambda^Ty + \nu(\mathbf1^Ty - 1).$$
The KKT conditions are 
\begin{gather*}
  \frac{\partial\mathcal L}{\partial y} = x - \mu(\log y - \mathbf1) - \lambda + \nu\mathbf1 = 0 \\
  \lambda \succeq 0, \quad y \succeq 0\\
  \mathbf1^T y = 1, \quad \lambda^Ty = 0
\end{gather*}
We have $y = \exp\l(\/1\mu\l(x-\mu\mathbf1 - \lambda + \nu\mathbf1\r)\r)$. Note that $y \succ 0$ and hence $\lambda = 0$. Moreover,
\begin{align*}
  1         & = \mathbf1^Ty\\
            & = \SUM{i=1}n \exp\l({\/1\mu(x_i-\mu+\nu)}\r) \\
            & = \exp\l({\/\nu\mu}\r) \SUM{i=1}n \exp\l({\/{x_i-\mu}\mu}\r)\\
  \nu       & = -\mu\log\l(\SUM{i=1}n \exp\l({\/{x_i-\mu}\mu}\r)\r)\\
  y_i       & = \exp\l(\frac{x_i-\mu+\nu}\mu\r) \\
  % y_i       & = \exp\l(\frac{x_i-\mu-\mu\log\l(\SUM{i=1}n e^{\frac{x_i-\mu}\mu\r)}}\mu\r) \\
  g^\ast(x) & = \SUM{i=1}n x_iy_i - \mu \SUM{i=1}n y_i\log y_i - \mu\log n \\
            & = \SUM{i=1}n y_i(x_i - \mu\log y_i) - \mu\log n \\
            & = \SUM{i=1}n y_i \l(\mu - \nu\r) - \mu \log n
\end{align*}


\subsubsection*{Problem 2. \textit{Projection on order cone.}}
(a) The Lagrangian for the constraint optimization problem (2) is 
$$\mathcal L(x, z) = \/12\|x-a\|_2^2 + z^TAx.$$
The KKT conditions are 
\begin{gather*}
  \frac{\partial\mathcal L}{\partial y} = x-a+A^Tz = 0\\
  z \succeq 0, \quad Ax \preceq 0 \\
  z^TAx = 0
\end{gather*}

\newcommand{\avg}{\mbox{avg}}
\newcommand{\cs}{\mbox{cs}}
\noindent(b) Observe that the algorithm sweeps through $1, \cdots, n$ and forms $\{i\}$ for each $i = 1, \cdots, n$ and merges them with successive sets based on a merging condition. This ensures the partitioning of $\{1, \cdots, n\}$ by $\beta_1, \cdots, \beta_l$. Due on the merging condition, we always have $\avg(a_{\beta_k}) < \avg(a_{\beta_{k+1}})$ because otherwise $\beta_{k+1}$ would have been merged into $\beta_k$. To show that $\cs(a_{\beta_k}) \succeq 0$, we need to show $cs(a_{\beta_k})_j \geq 0$ for $j \leq |\beta_k|$ except the last entry. Denote the subset of $\beta_k$ containing it's first $j$ indices by $\beta_k(j)$. Fix $j < |\beta_k|$, use the merging condition for the inequalities in the following derivation:
\begin{align*}
  \cs(a_{\beta_k})_j & = \SUM{i\in\beta_k(j)}{} a_i - j \avg(a_{\beta_k}) \\
                     & = j(\avg(a_{\beta_k(j)}) - \avg(a_{\beta_k})) \\
                     & \geq j(\avg(a_{\beta_k(j+1)}) - \avg(a_{\beta_k})) \\
                     & \geq \cdots \geq j(\avg(a_{\beta_k(|\beta_k|)} - \avg(a_{\beta_k}) = 0.
\end{align*}
(c) We showed $Ax\preceq 0$, or equivalently $x_1 \leq \cdots \leq x_n$ in ii, $z\succeq 0$ in iii. Observe that for $i = 1, \cdots, n-1$, 
$$(Ax)_i = \l\{
  \begin{array}{cl}
    \avg(a_{\beta_k}) - \avg(a_{\beta_{k+1}}),& i = \max\beta_k \\
    0,& \mbox{otherwise}
 \end{array} 
 \r., \qquad z_i = \l\{
   \begin{array}{cl}
     0,& i = \max\beta_k \\
     \mbox{(some value)},& \mbox{otherwise}
   \end{array}
   \r..$$
From here we deduce that $z^TAx = 0$ indeed. Moreover, observe that for $i = 1, \cdots, n-1$,
$$(A^Tz)_i = z_i - z_{i+1} = a_i - \avg(a_{\beta_k}), i \in \beta_k;$$
therefore, $x + A^Tz = a$. \\
\\
(d) The algorithm can be implemented at $\mathcal O(n)$ by keeping track of the average of each $\beta_k$: in each of the $n$ iterations described at the top of page 2, one compare $\avg(a_{\beta_{l-1}})$ with $a_i$, if the merging condition is satisfied, keep track of the index and update the average by 
$$\avg(a_{\beta_{l-1}}) = \frac{|\beta_{l-1}|\avg(a_{\beta_{l-1}}) + a_i}{|\beta_{l-1}| + 1}.$$
\end{document}



