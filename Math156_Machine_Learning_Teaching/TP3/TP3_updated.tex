\documentclass[12pt,a4paper]{article}

% \usepackage[utf8]{inputenc}
% \DeclareUnicodeCharacter{1D12A}{\doublesharp}
% \DeclareUnicodeCharacter{2693}{\anchor}
% \usepackage{dingbat}
% \DeclareRobustCommand\dash\unskip\nobreak\thinspace{\textemdash\allowbreak\thinspace\ignorespaces}
\usepackage[top=2in, bottom=1in, left=1in, right=1in]{geometry}
%\usepackage{fullpage}

\usepackage{tabularx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{fancyhdr}\pagestyle{fancy}\rhead{University of California -- Los Angeles Spring 2020}\lhead{Math 156 Term Project 3}

\usepackage{enumitem}

\usepackage{amsmath,amssymb,amsthm,amsfonts,microtype,stmaryrd}
	%{mathtools,wasysym,yhmath}

\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\fgreen}[1]{\textcolor{ForestGreen}{#1}}

\usepackage{mdframed}
	%\newtheorem{mdexample}{Example}
	\definecolor{warmgreen}{rgb}{0.8,0.9,0.85}
	% --Example:
	% \begin{center}
	% \begin{minipage}{0.7\textwidth}
	% \begin{mdframed}[backgroundcolor=warmgreen, 
	% skipabove=4pt,skipbelow=4pt,hidealllines=true, 
	% topline=false,leftline=false,middlelinewidth=10pt, 
	% roundcorner=10pt] 
	%%%% --CONTENTS-- %%%%
	% \end{mdframed}\end{minipage}\end{center}	

\usepackage{graphicx} \graphicspath{{}}
	% --Example:
	% \includegraphics[scale=0.5]{picture name}
%\usepackage{caption} %%% --some awful package to make caption...

\usepackage{hyperref}\hypersetup{linktocpage,colorlinks}\hypersetup{citecolor=black,filecolor=black,linkcolor=black,urlcolor=blue,breaklinks=true}

%%% --Text Fonts
%\usepackage{times} %%% --Times New Roman for LaTeX
%\usepackage{fontspec}\setmainfont{Times New Roman} %%% --Times New Roman; XeLaTeX only

%%% --Math Fonts
\renewcommand{\v}[1]{\ifmmode\mathbf{#1}\fi}
%\renewcommand{\mbf}[1]{\mathbf{#1}} %%% --vector
%\newcommand{\ca}[1]{\mathcal{#1}} %%% --"bigO"
%\newcommand{\bb}[1]{\mathbb{#1}} %%% --"Natural, Real numbers"
%\newcommand{\rom}[1]{\romannumeral{#1}} %%% --Roman numbers

%%% --Quick Arrows
\newcommand{\ra}[1]{\ifnum #1=1\rightarrow\fi\ifnum #1=2\Rightarrow\fi\ifnum #1=3\Rrightarrow\fi\ifnum #1=4\rightrightarrows\fi\ifnum #1=5\rightleftarrows\fi\ifnum #1=6\mapsto\fi\ifnum #1=7\iffalse\fi\fi\ifnum #1=8\twoheadrightarrow\fi\ifnum #1=9\rightharpoonup\fi\ifnum #1=0\rightharpoondown\fi}

%\newcommand{\la}[1]{\ifnum #1=1\leftarrow\fi\ifnum #1=2\Leftarrow\fi\ifnum #1=3\Lleftarrow\fi\ifnum #1=4\leftleftarrows\fi\ifnum #1=5\rightleftarrows\fi\ifnum #1=6\mapsfrom\ifnum #1=7\iffalse\fi\fi\ifnum #1=8\twoheadleftarrow\fi\ifnum #1=9\leftharpoonup\fi\ifnum #1=0\leftharpoondown\fi}

%\newcommand{\ua}[1]{\ifnum #1=1\uparrow\fi\ifnum #1=2\Uparrow\fi}
%\newcommand{\da}[1]{\ifnum #1=1\downarrow\fi\ifnum #1=2\Downarrow\fi}

%%% --Special Editor Config
\renewcommand{\ni}{\noindent}
\newcommand{\onum}[1]{\raisebox{.5pt}{\textcircled{\raisebox{-1pt} {#1}}}}

\newcommand{\claim}[1]{\underline{``{#1}":}}

\renewcommand{\l}{\left}\renewcommand{\r}{\right}

\newcommand{\casebrak}[4]{\left \{ \begin{array}{ll} {#1},&{#2}\\{#3},&{#4} \end{array} \right.}
%\newcommand{\ttm}[4]{\l[\begin{array}{cc}{#1}&{#2}\\{#3}&{#4}\end{array}\r]} %two-by-two-matrix
%\newcommand{\tv}[2]{\l[\begin{array}{c}{#1}\\{#2}\end{array}\r]}

\def\dps{\displaystyle}

\let\italiccorrection=\/
\def\/{\ifmmode\expandafter\frac\else\italiccorrection\fi}


%%% --General Math Symbols
\def\bc{\because}
\def\tf{\therefore}

%%% --Frequently used OPERATORS shorthand
\newcommand{\INT}[2]{\int_{#1}^{#2}}
% \newcommand{\UPINT}{\bar\int}
% \newcommand{\UPINTRd}{\overline{\int_{\bb R ^d}}}
\newcommand{\SUM}[2]{\sum\limits_{#1}^{#2}}
\newcommand{\PROD}[2]{\prod\limits_{#1}^{#2}}
\newcommand{\CUP}[2]{\bigcup\limits_{#1}^{#2}}
\newcommand{\CAP}[2]{\bigcap\limits_{#1}^{#2}}
% \newcommand{\SUP}[1]{\sup\limits_{#1}}
% \newcommand{\INF}[1]{\inf\limits_{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\pd}[2]{\frac{\partial{#1}}{\partial{#2}}}
\def\tr{\text{tr}}

\renewcommand{\o}{\circ}
\newcommand{\x}{\times}
\newcommand{\ox}{\otimes}

\newcommand\ie{{\it i.e. }}
\newcommand\wrt{{w.r.t. }}
\newcommand\dom{\mathbf{dom\:}}

%%% --Frequently used VARIABLES shorthand
\newcommand{\R}{\ifmmode\mathbb R\fi}
\newcommand{\N}{\ifmmode\mathbb N\fi}
\renewcommand{\O}{\mathcal{O}}
\newcommand{\w}{\wedge}
\newcommand{\ome}{\omega}
\newcommand{\lam}{\lambda}
\newcommand{\im}{\mbox{im}}

\newcommand{\B}{\mathcal B}
\newcommand{\var}{\mbox{var}}
\newcommand{\E}{\mathbb{E}}

%\usepackage{showkeys}
\newcommand{\ReLu}{\text{ReLu}}
\newcommand{\RELU}{\text{\textbf{ReLu}}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\title{Math 156 Term Project 3}
\author{\textbf{Deadline:} Monday May 18th 11:59pm}
\date{}
\maketitle

\subsubsection*{Problem 1}
ReLu function is a common activation function for neural networks. It is given by
\begin{equation*}
	\ReLu(x) = \l\{ 
		\begin{array}{ll}
		x, & x \geq 0\\
		0, & x < 0.
		\end{array}
		% {x, x \geq 0 \atop 0, x < 0.}
		\r.
\end{equation*}
% For vector valued input $\v x \in \R^K$, simply apply it to each of its entry,
% \begin{equation*}
% 	\RELU(\v x)_i = \l\{ 
% 		\begin{array}{ll}
% 		x_i, & x_i \geq 0\\
% 		0, & x_i < 0.
% 		\end{array}
% 		\r.
% \end{equation*}
Note that it is differentiable except at input $x = 0$. Show that $\ReLu: \R \ra1 \R$ is a convex function. (10 points) Show that the perceptron criterion for binary classification problem
\begin{equation*}
	E_P(\v w) = \SUM{i=1}N \ReLu(- \v w^T \boldsymbol\phi(\v x^{(i)}) t^{(i)})
\end{equation*}
is convex in $\v w$. Here the data set is given by $\mathcal D = \{(\v x^{(i)}, t^{(i)})\}_{i=1}^N$ where $t^{(i)} = 1$ if $i \in C_{[1]}$ and $t^{(i)} = -1$ if $i \in C_{[2]}$. (20 points)


\subsubsection*{Problem 2}
In convex optimization, we can use \textit{subderivatives} to find descending directions when the objective function is not differentiable. The \textit{subdifferential} of a convex function $f: \R^K \ra1 \R$ at point $\v x\in \R^K$ is defined to be the set 
\begin{equation*}
	\partial f(\v x) = \{ \v p \in \R^K \vert \text{ for all } \v y \in \R^K, f(\v y) \geq f(\v x) + \v p^T (\v y - \v x) \}.
\end{equation*}
Elements of this set are called \textit{subderivatives} (or \textit{subgradient}). Show that any nonzero subderivative is an ascending direction, that is, for any vector $\v p \in \partial f(\v x), \v p \neq \v 0$ and any positive scalar $\eta > 0$,  
\begin{equation*}
	f(\v x + \eta\v p) > f(\v x). \text{ (20 points)}
\end{equation*}

\subsubsection*{Problem 3}
In textbook \S4.1.7, we introduced a stochastic gradient descent method to minimize the perceptron criterion. Alternatively, we can use the subdifferential of $\ReLu$ (as defined in Problem 1) combined with backpropagation to find the optimal weights $\v W^{(l)}$ and biases $\v v^{(l)}$.
Show that 
\begin{align*}
	x > 0 \ra2 \partial\ReLu(x) &= \{1\} \\
	x = 0 \ra2 \partial\ReLu(0) &= \{p \in \R \ \vert\ 0 \leq p \leq 1 \} \\
	x < 0 \ra2 \partial\ReLu(x) &= \{0\}.
\end{align*}
In particular, we see that the subgradient $\partial f(\v x)$ at $\v x$ is a singleton set containing $\nabla f(\v x)$ when the function $f$ is differentiable at $\v x$. (20 points) \\


\subsubsection*{Problem 4}
% big O notation
% If a function $f: \N \ra1 \N$ satisfies that 
% \begin{equation*}
% 	f(n) \leq C g(n)
% \end{equation*}
% for all $n$ sufficiently large with some constant $C>0$ and a function $g: \N \ra1 \N$, we denote $f(n) = \mathcal O(g(n))$, meaning that \textit{the limit behavior of $f$ is bounded by a multiple of $g$.} For example, any second degree polynomial $f(n) = an^2 + bn + c$ is $\mathcal O(n^2)$ because $f(n) \leq |a+b+c|n^2$ as long as $n$ is sufficiently large.
In this problem, you need the notion of \textit{big-O notation}. You can seek help from \href{https://en.wikipedia.org/wiki/Big_O_notation}{Wikipedia} to familiarize yourself with it. Consider the $M$-layer perceptron network,
\begin{equation*}
	\mathbf y(\v x, \v W^{(1)}, \cdots, \v W^{(M)}, \v v^{(1)}, \cdots, \v v^{(M)}) = \sigma \o \psi^{(M)} \o h \o \psi^{(M-1)} \o \cdots \o h \o \psi^{(1)} (\v x),
\end{equation*}
where $h = \tanh$ is the activation function used for each layer, and the affine map for layer $l$ is given by $\psi^{(l)}(\v z) = \v v^{(l)} + \v W^{(l)} \v z$.
Denote the (hidden) units by
\begin{align*}
	\mathbf z^{(0)} &= \v x \in \R^{K_0} \\
	\mathbf a^{(1)} &= \v v^{(1)} + \v W^{(1)} \v z^{(0)} \in \R^{K_1} \\
	\mathbf z^{(1)} &= h(\v a^{(1)}) \in \R^{K_1} \\
	\mathbf a^{(2)} &= \v v^{(2)} + \v W^{(2)} \v z^{(1)} \in \R^{K_2} \\
	\mathbf z^{(2)} &= h(\v a^{(2)}) \in \R^{K_2} \\
	\vdots \\
	\mathbf a^{(M)} &= \v v^{(M)} + \v W^{(M)} \v z^{(M-1)} \in \R^{K_M} \\
	\mathbf z^{(M)} & = \mathbf y(\v x, \v W^{(1)}, \cdots, \v W^{(M)}, \v v^{(1)}, \cdots, \v v^{(M)}) = \sigma(\v a^{(M)}).
\end{align*}
Suppose for simplicity that $K_1 = K_2 = \cdots = K_M = K$, express the cost of backpropagation in terms of the $K$ and $M$. (30 points)
\end{document}



